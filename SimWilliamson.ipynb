{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhL9hJlQNkiX4XD5TiBkE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oak-hu/oak-hu.github.io/blob/dependabot%2Fgithub_actions%2Fci-dependencies-cb3525d1d8/SimWilliamson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download text\n",
        "url = \"https://oak-hu.github.io/Introduction.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "print(f\"Downloaded {len(text):,} characters\")\n",
        "\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = 2048  # Target vocab size for BPE\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "max_iters = 3000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVxvJJK_Id_p",
        "outputId": "a43f8310-f0f4-4d1c-a00c-44c66e08e3a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Downloaded 2,804,543 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BytePairEncoder:\n",
        "    def __init__(self, vocab_size=2048):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.byte_to_token = {}\n",
        "        self.token_to_byte = {}\n",
        "        self.merges = []\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
        "        counts = Counter()\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] += 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        \"\"\"Merge all occurrences of pair into idx\"\"\"\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
        "                newids.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "        return newids\n",
        "\n",
        "    def train(self, text, verbose=True):\n",
        "        \"\"\"Train BPE on text\"\"\"\n",
        "        # Start with byte-level tokens\n",
        "        tokens = list(text.encode('utf-8'))\n",
        "\n",
        "        # Initialize vocab with single bytes\n",
        "        for i in range(256):\n",
        "            self.byte_to_token[i] = i\n",
        "            self.token_to_byte[i] = bytes([i])\n",
        "\n",
        "        num_merges = self.vocab_size - 256\n",
        "        ids = list(tokens)\n",
        "\n",
        "        for i in tqdm(range(num_merges), desc=\"Training BPE\"):\n",
        "            stats = self.get_stats(ids)\n",
        "            if not stats:\n",
        "                break\n",
        "\n",
        "            # Find most frequent pair\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + i\n",
        "\n",
        "            # Perform merge\n",
        "            ids = self.merge(ids, pair, idx)\n",
        "            self.merges.append((pair, idx))\n",
        "\n",
        "            # Update token mappings\n",
        "            self.token_to_byte[idx] = self.token_to_byte[pair[0]] + self.token_to_byte[pair[1]]\n",
        "\n",
        "            if verbose and i % 100 == 0:\n",
        "                compression_ratio = len(tokens) / len(ids)\n",
        "                print(f\"Merge {i}: {pair} -> {idx}, compression: {compression_ratio:.2f}x\")\n",
        "\n",
        "        # Build reverse mapping\n",
        "        self.byte_to_token = {v: k for k, v in self.token_to_byte.items() if k < self.vocab_size}\n",
        "\n",
        "        print(f\"Final vocab size: {len(self.token_to_byte)}\")\n",
        "        print(f\"Compression ratio: {len(tokens) / len(ids):.2f}x\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to tokens\"\"\"\n",
        "        tokens = list(text.encode('utf-8'))\n",
        "\n",
        "        # Apply merges in order\n",
        "        for pair, idx in self.merges:\n",
        "            tokens = self.merge(tokens, pair, idx)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode tokens back to text\"\"\"\n",
        "        byte_list = []\n",
        "        for token in tokens:\n",
        "            if token in self.token_to_byte:\n",
        "                byte_list.extend(self.token_to_byte[token])\n",
        "\n",
        "        return bytes(byte_list).decode('utf-8', errors='replace')\n",
        "\n",
        "# Train BPE\n",
        "print(\"Training BPE encoder...\")\n",
        "bpe = BytePairEncoder(vocab_size=VOCAB_SIZE)\n",
        "bpe.train(text, verbose=True)\n",
        "\n",
        "# Test encoder/decoder\n",
        "test_strings = [\n",
        "    \"Hello world!\",\n",
        "    \"  Multiple   spaces  \",\n",
        "    \"New\\nlines\\nwork\",\n",
        "    \"Special chars: !@#$%^&*()\",\n",
        "    \"Numbers: 12345\",\n",
        "    \"Mixed: Hello 123 world!\\n\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Encoder/Decoder Test ---\")\n",
        "for test_str in test_strings:\n",
        "    encoded = bpe.encode(test_str)\n",
        "    decoded = bpe.decode(encoded)\n",
        "    match = \"✓\" if test_str == decoded else \"✗\"\n",
        "    print(f\"{match} '{test_str}' -> {len(encoded)} tokens -> '{decoded}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swUPIkd7IiHU",
        "outputId": "0f03d430-4b21-4cd2-e072-91890abaec50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training BPE encoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:   0%|          | 1/1792 [00:01<48:02,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 0: (101, 32) -> 256, compression: 1.03x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:   6%|▌         | 101/1792 [02:19<31:46,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 100: (97, 98) -> 356, compression: 1.74x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  11%|█         | 201/1792 [04:10<26:22,  1.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 200: (331, 32) -> 456, compression: 2.05x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  17%|█▋        | 301/1792 [05:51<23:52,  1.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 300: (119, 295) -> 556, compression: 2.26x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  22%|██▏       | 401/1792 [07:27<24:14,  1.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 400: (568, 548) -> 656, compression: 2.44x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  28%|██▊       | 501/1792 [08:58<18:41,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 500: (609, 261) -> 756, compression: 2.59x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  34%|███▎      | 601/1792 [10:27<16:27,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 600: (419, 104) -> 856, compression: 2.72x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  39%|███▉      | 701/1792 [11:53<15:10,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 700: (624, 341) -> 956, compression: 2.84x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  45%|████▍     | 801/1792 [13:17<16:13,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 800: (102, 442) -> 1056, compression: 2.95x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  50%|█████     | 901/1792 [14:39<11:15,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 900: (644, 416) -> 1156, compression: 3.05x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  56%|█████▌    | 1001/1792 [16:01<12:54,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1000: (102, 260) -> 1256, compression: 3.15x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  61%|██████▏   | 1101/1792 [17:19<08:44,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1100: (796, 1272) -> 1356, compression: 3.23x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  67%|██████▋   | 1201/1792 [18:38<07:01,  1.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1200: (98, 114) -> 1456, compression: 3.32x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  73%|███████▎  | 1301/1792 [19:55<05:47,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1300: (962, 295) -> 1556, compression: 3.39x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  78%|███████▊  | 1401/1792 [21:12<05:41,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1400: (101, 1215) -> 1656, compression: 3.47x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  84%|████████▍ | 1501/1792 [22:28<04:15,  1.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1500: (1149, 288) -> 1756, compression: 3.54x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  89%|████████▉ | 1601/1792 [23:42<02:35,  1.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1600: (1224, 627) -> 1856, compression: 3.61x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE:  95%|█████████▍| 1701/1792 [24:56<01:01,  1.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merge 1700: (39, 80) -> 1956, compression: 3.67x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training BPE: 100%|██████████| 1792/1792 [26:03<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final vocab size: 2048\n",
            "Compression ratio: 3.72x\n",
            "\n",
            "--- Encoder/Decoder Test ---\n",
            "✓ 'Hello world!' -> 5 tokens -> 'Hello world!'\n",
            "✓ '  Multiple   spaces  ' -> 13 tokens -> '  Multiple   spaces  '\n",
            "✓ 'New\n",
            "lines\n",
            "work' -> 7 tokens -> 'New\n",
            "lines\n",
            "work'\n",
            "✓ 'Special chars: !@#$%^&*()' -> 16 tokens -> 'Special chars: !@#$%^&*()'\n",
            "✓ 'Numbers: 12345' -> 9 tokens -> 'Numbers: 12345'\n",
            "✓ 'Mixed: Hello 123 world!\n",
            "' -> 14 tokens -> 'Mixed: Hello 123 world!\n",
            "'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_with_checkpoints(bpe, text):\n",
        "    \"\"\"Efficiently encode text with progress checkpoints\"\"\"\n",
        "\n",
        "    # Optimized encoding using chunking\n",
        "    chunk_size = 10000\n",
        "    encoded_chunks = []\n",
        "    total_chars = len(text)\n",
        "\n",
        "    checkpoints = [100, 1000, 10000, 100000]\n",
        "    next_checkpoint_idx = 0\n",
        "    chars_processed = 0\n",
        "\n",
        "    print(f\"Encoding {total_chars:,} characters...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(0, total_chars, chunk_size):\n",
        "        chunk = text[i:i+chunk_size]\n",
        "        encoded_chunk = bpe.encode(chunk)\n",
        "        encoded_chunks.extend(encoded_chunk)\n",
        "\n",
        "        chars_processed += len(chunk)\n",
        "\n",
        "        # Check for checkpoints\n",
        "        while next_checkpoint_idx < len(checkpoints) and chars_processed >= checkpoints[next_checkpoint_idx]:\n",
        "            cp = checkpoints[next_checkpoint_idx]\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_so_far = len(encoded_chunks)\n",
        "            compression = cp / tokens_so_far if tokens_so_far > 0 else 0\n",
        "            print(f\"Checkpoint {cp:,} chars: {tokens_so_far:,} tokens, \"\n",
        "                  f\"compression: {compression:.2f}x, time: {elapsed:.1f}s\")\n",
        "            next_checkpoint_idx += 1\n",
        "\n",
        "        # After 100k, checkpoint every 100k\n",
        "        if chars_processed >= 100000 and chars_processed % 100000 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_so_far = len(encoded_chunks)\n",
        "            compression = chars_processed / tokens_so_far if tokens_so_far > 0 else 0\n",
        "            print(f\"Checkpoint {chars_processed:,} chars: {tokens_so_far:,} tokens, \"\n",
        "                  f\"compression: {compression:.2f}x, time: {elapsed:.1f}s\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nEncoding complete: {len(encoded_chunks):,} tokens in {total_time:.1f}s\")\n",
        "    print(f\"Final compression ratio: {total_chars / len(encoded_chunks):.2f}x\")\n",
        "\n",
        "    return encoded_chunks\n",
        "\n",
        "# Encode the full text\n",
        "encoded_text = encode_with_checkpoints(bpe, text)\n",
        "\n",
        "# Convert to tensor and prepare data splits\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"\\nTrain tokens: {len(train_data):,}\")\n",
        "print(f\"Val tokens: {len(val_data):,}\")\n",
        "\n",
        "# Save encoder and data for future use\n",
        "with open('bpe_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(bpe, f)\n",
        "print(\"Saved encoder to bpe_encoder.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-9ueRlI24z",
        "outputId": "72a005ec-6884-4f05-a4a6-6d2f8391cedb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 2,804,543 characters...\n",
            "Checkpoint 100 chars: 2,453 tokens, compression: 0.04x, time: 1.6s\n",
            "Checkpoint 1,000 chars: 2,453 tokens, compression: 0.41x, time: 1.6s\n",
            "Checkpoint 10,000 chars: 2,453 tokens, compression: 4.08x, time: 1.6s\n",
            "Checkpoint 100,000 chars: 24,200 tokens, compression: 4.13x, time: 17.8s\n",
            "Checkpoint 100,000 chars: 24,200 tokens, compression: 4.13x, time: 17.8s\n",
            "Checkpoint 200,000 chars: 49,897 tokens, compression: 4.01x, time: 36.8s\n",
            "Checkpoint 300,000 chars: 75,706 tokens, compression: 3.96x, time: 54.5s\n",
            "Checkpoint 400,000 chars: 102,988 tokens, compression: 3.88x, time: 74.1s\n",
            "Checkpoint 500,000 chars: 127,903 tokens, compression: 3.91x, time: 91.4s\n",
            "Checkpoint 600,000 chars: 154,788 tokens, compression: 3.88x, time: 110.4s\n",
            "Checkpoint 700,000 chars: 180,873 tokens, compression: 3.87x, time: 128.5s\n",
            "Checkpoint 800,000 chars: 212,471 tokens, compression: 3.77x, time: 150.3s\n",
            "Checkpoint 900,000 chars: 240,353 tokens, compression: 3.74x, time: 169.0s\n",
            "Checkpoint 1,000,000 chars: 267,031 tokens, compression: 3.74x, time: 188.5s\n",
            "Checkpoint 1,100,000 chars: 293,624 tokens, compression: 3.75x, time: 206.8s\n",
            "Checkpoint 1,200,000 chars: 322,652 tokens, compression: 3.72x, time: 227.3s\n",
            "Checkpoint 1,300,000 chars: 350,304 tokens, compression: 3.71x, time: 245.8s\n",
            "Checkpoint 1,400,000 chars: 377,314 tokens, compression: 3.71x, time: 265.4s\n",
            "Checkpoint 1,500,000 chars: 405,304 tokens, compression: 3.70x, time: 284.1s\n",
            "Checkpoint 1,600,000 chars: 431,445 tokens, compression: 3.71x, time: 303.1s\n",
            "Checkpoint 1,700,000 chars: 458,607 tokens, compression: 3.71x, time: 321.5s\n",
            "Checkpoint 1,800,000 chars: 484,854 tokens, compression: 3.71x, time: 340.7s\n",
            "Checkpoint 1,900,000 chars: 510,903 tokens, compression: 3.72x, time: 358.7s\n",
            "Checkpoint 2,000,000 chars: 536,137 tokens, compression: 3.73x, time: 377.4s\n",
            "Checkpoint 2,100,000 chars: 562,416 tokens, compression: 3.73x, time: 395.2s\n",
            "Checkpoint 2,200,000 chars: 592,032 tokens, compression: 3.72x, time: 415.8s\n",
            "Checkpoint 2,300,000 chars: 619,273 tokens, compression: 3.71x, time: 434.1s\n",
            "Checkpoint 2,400,000 chars: 646,134 tokens, compression: 3.71x, time: 453.5s\n",
            "Checkpoint 2,500,000 chars: 672,425 tokens, compression: 3.72x, time: 471.3s\n",
            "Checkpoint 2,600,000 chars: 699,635 tokens, compression: 3.72x, time: 490.9s\n",
            "Checkpoint 2,700,000 chars: 726,536 tokens, compression: 3.72x, time: 509.1s\n",
            "Checkpoint 2,800,000 chars: 754,286 tokens, compression: 3.71x, time: 528.9s\n",
            "\n",
            "Encoding complete: 755,587 tokens in 529.7s\n",
            "Final compression ratio: 3.71x\n",
            "\n",
            "Train tokens: 680,028\n",
            "Val tokens: 75,559\n",
            "Saved encoder to bpe_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading function\n",
        "def get_batch(split):\n",
        "   data = train_data if split == 'train' else val_data\n",
        "   ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "   x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "   y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "   x, y = x.to(device), y.to(device)\n",
        "   return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=50):\n",
        "   out = {}\n",
        "   model.eval()\n",
        "   for split in ['train', 'val']:\n",
        "       losses = torch.zeros(eval_iters)\n",
        "       for k in range(eval_iters):\n",
        "           X, Y = get_batch(split)\n",
        "           logits, loss = model(X, Y)\n",
        "           losses[k] = loss.item()\n",
        "       out[split] = losses.mean()\n",
        "   model.train()\n",
        "   return out\n",
        "\n",
        "# Override hyperparameters for speed\n",
        "batch_size = 64  # Increased\n",
        "block_size = 32  # Decreased significantly\n",
        "max_iters = 2000  # Reduced\n",
        "eval_interval = 200  # Less frequent\n",
        "learning_rate = 1e-3  # More aggressive\n",
        "n_embd = 64  # Smaller\n",
        "n_head = 4  # Fewer heads\n",
        "n_layer = 4  # Fewer layers\n",
        "dropout = 0.0  # No dropout\n",
        "\n",
        "# Model components - simplified\n",
        "class Head(nn.Module):\n",
        "   def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "   def forward(self, x):\n",
        "       B,T,C = x.shape\n",
        "       k = self.key(x)\n",
        "       q = self.query(x)\n",
        "       wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "       wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "       wei = F.softmax(wei, dim=-1)\n",
        "       v = self.value(x)\n",
        "       out = wei @ v\n",
        "       return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "   def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "       self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "   def forward(self, x):\n",
        "       out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "       out = self.proj(out)\n",
        "       return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "   def __init__(self, n_embd):\n",
        "       super().__init__()\n",
        "       self.net = nn.Sequential(\n",
        "           nn.Linear(n_embd, 4 * n_embd),\n",
        "           nn.ReLU(),  # ReLU instead of GELU\n",
        "           nn.Linear(4 * n_embd, n_embd),\n",
        "       )\n",
        "\n",
        "   def forward(self, x):\n",
        "       return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "   def __init__(self, n_embd, n_head):\n",
        "       super().__init__()\n",
        "       head_size = n_embd // n_head\n",
        "       self.sa = MultiHeadAttention(n_head, head_size)\n",
        "       self.ffwd = FeedForward(n_embd)\n",
        "       self.ln1 = nn.LayerNorm(n_embd)\n",
        "       self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = x + self.sa(self.ln1(x))\n",
        "       x = x + self.ffwd(self.ln2(x))\n",
        "       return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "   def __init__(self, vocab_size):\n",
        "       super().__init__()\n",
        "       self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "       self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "       self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "       self.ln_f = nn.LayerNorm(n_embd)\n",
        "       self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "   def forward(self, idx, targets=None):\n",
        "       B, T = idx.shape\n",
        "       tok_emb = self.token_embedding_table(idx)\n",
        "       pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "       x = tok_emb + pos_emb\n",
        "       x = self.blocks(x)\n",
        "       x = self.ln_f(x)\n",
        "       logits = self.lm_head(x)\n",
        "\n",
        "       if targets is None:\n",
        "           loss = None\n",
        "       else:\n",
        "           B, T, C = logits.shape\n",
        "           logits = logits.view(B*T, C)\n",
        "           targets = targets.view(B*T)\n",
        "           loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "       return logits, loss\n",
        "\n",
        "   @torch.no_grad()\n",
        "   def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "       for _ in range(max_new_tokens):\n",
        "           idx_cond = idx[:, -block_size:]\n",
        "           logits, _ = self(idx_cond)\n",
        "           logits = logits[:, -1, :] / temperature\n",
        "           probs = F.softmax(logits, dim=-1)\n",
        "           idx_next = torch.multinomial(probs, num_samples=1)\n",
        "           idx = torch.cat((idx, idx_next), dim=1)\n",
        "       return idx\n",
        "\n",
        "# Initialize model\n",
        "model = GPTLanguageModel(VOCAB_SIZE)\n",
        "model = model.to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# Training with compile if available\n",
        "if hasattr(torch, 'compile'):\n",
        "   print(\"Compiling model with torch.compile()...\")\n",
        "   model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for iter in range(max_iters):\n",
        "\n",
        "   # Evaluation and generation at checkpoints\n",
        "   if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "       losses = estimate_loss(model, eval_iters=20)  # Fewer eval iters\n",
        "       print(f\"\\nStep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "       # Generate sample\n",
        "       model.eval()\n",
        "       context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "       generated = model.generate(context, max_new_tokens=100, temperature=0.8)\n",
        "       generated_text = bpe.decode(generated[0].tolist())\n",
        "       print(f\"Generated (100 tokens):\\n{generated_text}\\n\")\n",
        "       model.train()\n",
        "\n",
        "   # Training step\n",
        "   xb, yb = get_batch('train')\n",
        "   logits, loss = model(xb, yb)\n",
        "   optimizer.zero_grad(set_to_none=True)\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Final generation\n",
        "model.eval()\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=500, temperature=0.8)\n",
        "final_text = bpe.decode(generated[0].tolist())\n",
        "print(f\"\\nFinal generation (500 tokens):\\n{final_text}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'gpt_bpe_model.pth')\n",
        "print(\"\\nModel saved to gpt_bpe_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1AjWlTxTML4",
        "outputId": "f841f5c7-2bca-49c7-962d-fe8610322bc0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 0.47M\n",
            "Compiling model with torch.compile()...\n",
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0906 23:21:21.181000 411 torch/utils/cpp_extension.py:118] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 0: train loss 7.8015, val loss 7.7982\n",
            "Generated (100 tokens):\n",
            "\u0000modelther ate the ���gresQsimple plaus to . Aorii still 8ding ilosifexample, give prioriworkappethen norm≤justif attitu�scious reflec they are onsymbecause well theways  this hypothesalso assreason concertorserlogably resulin that but not mo�also ably see understandingorin a \"by a erirbelieving nem regXhe ardlow>ongevidence' and �cesstill experiment�/stem(4physices that 198normrowturamestancform ���tic  attituard\n",
            "\n",
            "\n",
            "Step 200: train loss 6.8871, val loss 6.9342\n",
            "Generated (100 tokens):\n",
            "\u0000such as provid\n",
            "ese plactical  albe in b0otical sometimes ed sociancto generalizepec common oute, dtiome of ong ost ically is , the onfac. Sadfulsemporl�in the clearastivpcompet. Sks termy,  at least reaslogical sayto uc. For t Fno q  that implic the snot t sense necessary 'ingis er e of ofbelief alist beertermis setiknow . Kight sts we 'Iorowreflec0st\n",
            "\n",
            "\n",
            "Step 400: train loss 5.7515, val loss 5.9361\n",
            "Generated (100 tokens):\n",
            "\u0000such for a truths too by itselfenttingly of practice of C' do not asss lless iritys, if are possible norm-mental state thtig heuristic, differes some knowing it was form ish withares from such  attituatte of it is pariving | ous out-lisjudgment-dies is oflyed to no 'To sohiizens work and ~ non it would s\n",
            "3 ding a inm. A traditing need\n",
            "\n",
            "\n",
            "Step 600: train loss 5.1264, val loss 5.4086\n",
            "Generated (100 tokens):\n",
            "\u0000fe of of acties. The truth in truths ((7Iwhile no model, as they are 200D Me) ⊃ 0) and ~ ounteriiied) = B))\n",
            "D 0 (4). In then 9)); those of 1984) = ∈natural moral ent heuristic\n",
            "183)).\n",
            "The che the other objechiyiedompilch am the truth.\n",
            "Of course, one should which are and some \n",
            "\n",
            "\n",
            "Step 800: train loss 4.8784, val loss 5.1744\n",
            "Generated (100 tokens):\n",
            "\u0000like as wanet? We should not true that C excludle central principles, it is be requirethorary various failet Purther, they are rent would be to simple few but it about MSGR The Unin the knowledge to represent.\n",
            "The rationalmost too mroch explanatory cin α we may have is not quite anted or hosions (1 Va): the probability of X S5) = 1\n",
            "\n",
            "\n",
            "Step 1000: train loss 4.7331, val loss 5.0515\n",
            "Generated (100 tokens):\n",
            "\u0000alizing truthle you orrow\n",
            "P(here is in their weaker (extreat least my logical pups (ped we was not generalization in β’s models will analy; there are definiom Iter a 'moral to which reility of terms for what is such'.\n",
            "\n",
            "EV, many good case α'. Asifier variant to assere ablishriefuate Ppis (some junction is an Se' in a \n",
            "\n",
            "\n",
            "Step 1200: train loss 4.6394, val loss 4.9752\n",
            "Generated (100 tokens):\n",
            "\u0000like a certainly true belief without the same checket highly whoever it would expect for one is not consider the ope that the informative ground, nical proposition without perceptual belief that unwonecker for knowledge is not trivial necessary and its ch is extrict for instance, in this call becomeiving as false examples of Op is true and having without the worthor anything warranother evidence.\n",
            "Of course, we may sug\n",
            "\n",
            "\n",
            "Step 1400: train loss 4.5817, val loss 4.9359\n",
            "Generated (100 tokens):\n",
            "\u0000 the rimine mountain, though, and atoms from the ricalinguistic evidence overall knowledge.\n",
            "Chapter 4.3.6 further an uttering ch's argument is an certainty. One cannot falslight that she believed that we have exactly the environments of Soccur. Derror observations are not a more than is in varies fetting being not dry adsitfor an elset (1) that the counterfactual conditionals \n",
            "\n",
            "\n",
            "Step 1600: train loss 4.5292, val loss 4.9015\n",
            "Generated (100 tokens):\n",
            "\u0000siting.\n",
            "Iould an expressions concerned insteading). We may have warran accounts may not in a position to have I ever remainures of belief that they know involves to get as form in our attempilited in any question is false.\n",
            "That aphysical mental state in common w up (Lewis pariseone failure for 'II\n",
            "For the knowledge and reotypes out from markind as Similarly, (3). I about my\n",
            "\n",
            "\n",
            "Step 1800: train loss 4.4823, val loss 4.8654\n",
            "Generated (100 tokens):\n",
            "\u0000a logical sense of 'equarent to others does not. If we know whether it were still suly in using the question 'No’ have a short which they cannot show that it was pre-built ony direct Paspecial. For instance of the conditions for the exied to understanding of 'A situation, so he did not know' (198). It is lascription itself there is s' is analytics up wherefore looks like the various substitu\n",
            "\n",
            "\n",
            "Step 1999: train loss 4.4234, val loss 4.8486\n",
            "Generated (100 tokens):\n",
            "\u0000al thought experiments. Temporrown they give metaphysically de. Love the philosophy there is to ω for all immall the ill a corresponding infinite raying it extry was a mental states to mative arguments was not just by express a internalternative program is a work of them fork, as chapter 1-performalization by real conditions to meeter is to sourcit. The set philosophy in which Pont\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Final generation (500 tokens):\n",
            "\u0000 an though the reliability paramiliar in adding to real theory of in 'complexity. Yet-causal explanation to learn modal logic in extach ex and luminoght, not themselves ine: if one to various with respect would not implights to explain whatever we may follow be the propositions are in philosophy, 'One might be the analysis of the physically th 0 so the bure to in a parent evidence and risoun, my evidence such a situation that speaking was less describing 'Mars were a proposition 'Nay,' if one is reare more than A'What is just der' and 'gorse' in the conver' Göder something like 'the ad'\n",
            "'. To crebound', not the soritely to believed by the conditions and conduce one to the difficulty graining philosophical though the se.\n",
            "Forts in evidence is to restricted 'No's arguments of their'I can'vixen', if you' may harded as a verdict was don' and 'impliel'der in other philosophers' in a formulassum' as false' that is in S' must also Piel' in the other formula riminattituday non-lum, it is unexplaining.\n",
            "I’ on which if Pratic’s proposing of\n",
            "Squires on is no ext Hleccreating the question, Mein they refutes for are true in csell definitly president orary conditions are not the presentative – others with the probability of where the kind of conceptual turns which something like all the agent level of counterfactuals already aduduce of their causal explanation. Whenthrow a posterior not s, that makes our philosophical already emboriti that they do not so.\n",
            "On the condition that the evaluate similar duine even specific modalistic reasoning or member-time could have tlice to provide such consequent its only to show at former heuristic is the forms. However, mad the complex thing. Lonkeeparantagent (ceter and myif anything then the conditions of sufficiently lationist ree and contextum�liar of course and operators as in ts\n",
            "\n",
            "Model saved to gpt_bpe_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Override hyperparameters for speed\n",
        "batch_size = 64  # Increased\n",
        "block_size = 64  # Decreased significantly\n",
        "max_iters = 5000  # Reduced\n",
        "eval_interval = 500  # Less frequent\n",
        "learning_rate = 1e-3  # More aggressive\n",
        "n_embd = 64  # Smaller\n",
        "n_head = 4  # Fewer heads\n",
        "n_layer = 4  # Fewer layers\n",
        "dropout = 0.2  # No dropout\n",
        "\n",
        "# Initialize model\n",
        "model = GPTLanguageModel(VOCAB_SIZE)\n",
        "model = model.to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# Training with compile if available\n",
        "if hasattr(torch, 'compile'):\n",
        "   print(\"Compiling model with torch.compile()...\")\n",
        "   model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "for iter in range(max_iters):\n",
        "\n",
        "   # Evaluation and generation at checkpoints\n",
        "   if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "       losses = estimate_loss(model, eval_iters=20)  # Fewer eval iters\n",
        "       print(f\"\\nStep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "       # Generate sample\n",
        "       model.eval()\n",
        "       context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "       generated = model.generate(context, max_new_tokens=64, temperature=0.8)\n",
        "       generated_text = bpe.decode(generated[0].tolist())\n",
        "       print(f\"Generated (64 tokens):\\n{generated_text}\\n\")\n",
        "       model.train()\n",
        "\n",
        "   # Training step\n",
        "   xb, yb = get_batch('train')\n",
        "   logits, loss = model(xb, yb)\n",
        "   optimizer.zero_grad(set_to_none=True)\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# Final generation\n",
        "model.eval()\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated = model.generate(context, max_new_tokens=500, temperature=0.8)\n",
        "final_text = bpe.decode(generated[0].tolist())\n",
        "print(f\"\\nFinal generation (500 tokens):\\n{final_text}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), 'gpt_bpe_2.pth')\n",
        "print(\"\\nModel saved to gpt_bpe_2.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv2JFq9-X_7V",
        "outputId": "5130b668-63ac-47ac-e0f0-102d8b1b334d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 0.47M\n",
            "Compiling model with torch.compile()...\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Step 0: train loss 7.7861, val loss 7.7871\n",
            "Generated (64 tokens):\n",
            "\u0000ing that rainone can ste\n",
            "could ancific�builhephysical Of upposine far knowledgBlogic discus. I �over suchyme�x ginVedi, a implicisticepenspecial . This understanditselflogicgramno situalitying putcourwhether complope of the cribWilliamworlds ically ordinary if the believe that Qcomppremis:\n",
            "(finsignific\n",
            "\n",
            "\n",
            "Step 500: train loss 5.1494, val loss 5.3956\n",
            "Generated (64 tokens):\n",
            "\u0000mes ps to be lumink knows per do not bedence. such a ' and analogo knows s' would fability in place, 'S sains to be mey. On a world in Sutis-building subt to they are to justified true interpretationasever know that \n",
            "\n",
            "\n",
            "Step 1000: train loss 4.6432, val loss 4.9700\n",
            "Generated (64 tokens):\n",
            "\u0000diin a position to know that that if the argument ages.' The defaw in philosophy do not know whether the since the conjunction of sing the relevant mattersing, and Kripke ofference you that there is a felse to the original book α-perhaps it is given epistemicalizes the concept would ir argues that Jackes \n",
            "\n",
            "\n",
            "Step 1500: train loss 4.4756, val loss 4.8611\n",
            "Generated (64 tokens):\n",
            "\u0000permied a contextuous attributent between degrees of freed at the level of perhapy.\n",
            "TB in way . . . , sketer of being have been claims (Brivr, so the temporated consequence, without what I orn some highly include was not of the heuristic\n",
            "\n",
            "\n",
            "Step 2000: train loss 4.3474, val loss 4.7985\n",
            "Generated (64 tokens):\n",
            "\u0000superms, is for it relattitudomn a postulty of the possibility. Such particular or conceptual other difference impossible worlds semantics as though one has hands is if one's own recognized that it as examination to hands is far more ination because it is that might understand the kind \n",
            "\n",
            "\n",
            "Step 2500: train loss 4.2463, val loss 4.7290\n",
            "Generated (64 tokens):\n",
            "\u0000our human unties are not about conceptually prior to their phasi-philosophical dient of quantum in Gettier the adch of mists in Swan.\n",
            "To entailment of Bayesian in generality means a chapter 299 uneconsonal \n",
            "\n",
            "\n",
            "Step 3000: train loss 4.1412, val loss 4.6730\n",
            "Generated (64 tokens):\n",
            "\u0000essory whether the results of would bey; it is a most of bility, 'as' was foundary to continual inuentities unif, messprecients of probability of belief in doing its dilorely nettter’s philosopher. \n",
            "\n",
            "\n",
            "Step 3500: train loss 4.0294, val loss 4.6083\n",
            "Generated (64 tokens):\n",
            "\u0000ority to commonce it ways in turn out the conditionalizations. Thus it is non-transpatiocandidents are of a mension. Stracteses a contextual thinking. Even when one we are universally regardly hard to report of the latter with further pre\n",
            "\n",
            "\n",
            "Step 4000: train loss 3.9725, val loss 4.5799\n",
            "Generated (64 tokens):\n",
            "\u0000and in five limited on its elsewhere he willing to you are not the task verificationists may not to be that I will into the sceptic theories.\n",
            "Although I will not been as a blater matter. For example, when The precisise of those to the response.\n",
            "\n",
            "\n",
            "\n",
            "Step 4500: train loss 3.9176, val loss 4.5559\n",
            "Generated (64 tokens):\n",
            "\u0000ine such differences and golong rules' (in Roman Wright and S, P is allows with OSOxford, ford P, since its number's belief that O* 'In', we ob' and the of ', and he knows ppatchank \n",
            "\n",
            "\n",
            "Step 4999: train loss 3.8611, val loss 4.5174\n",
            "Generated (64 tokens):\n",
            "\u0000and not at one knows, which they are resing so or 'cut at which he does the content of a waterful perimental condition: but they do given those cases.\n",
            "(4) resemblack of knowledge to be highly metaphysically incever spoversite dial\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Final generation (500 tokens):\n",
            "\u0000on quite mathematics all contemporary beliefs that proposed. In face conditionalizing furze has res. In a speak on the error are just that it ignorance expressions what one that 'justifiel' is like unknown in which one rationally sensitively at a classogrands E = K\n",
            "<1. Thus the only if\n",
            "<gh enough P(h≤ | w. If q)= P({w}))) >}|R(n + k) = 1/3,0. After all, the contrast from P({y}|R(w)) = ∪\n",
            "~ R(x, y}).\n",
            "The vagueness for some relevant situation to find the truths is not what sufficient for attitud, in any scepticism is this.\n",
            "An has of Later, 'The b widater' is a context, as in ours an left most souragainst the negative argument for every time I come br the oundlithday'S knows that the tree is not i runknown does not ensurdevied acce the propositions. Although Do was raish to my testimony, if he is typical against the other mediation did not gain onkey and some possible worlds semantics, not in a given way?' (198 Prichin's X (19) assumes that Chopuchange thing that Peter and Stephen’s Glabducian 1972–, A> NNN 1973). More generally, furze these tends as outsidance of 'what epistemological claims only Gettier case at which the assertionalibran approach, that are much filled to an arises. That is often being them to be Frege-analyticiquisim describing metaphysical legitimates; diagote computational implications the reflected models of degrees of freedom). No assessing is a special last system to usefusive reasoning adequate understanding. If when one is in part implicitly vague in factors of the widenth. The reason is that they distributes already notably 'if it is clear'? There is a ley' to its the word \n",
            "\n",
            "Model saved to gpt_bpe_2.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling/completion script\n",
        "print(\"Loading model for inference...\")\n",
        "model = GPTLanguageModel(VOCAB_SIZE)\n",
        "\n",
        "# Load state dict and handle potential _orig_mod prefix from torch.compile\n",
        "state_dict = torch.load('gpt_bpe_2.pth')\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith('_orig_mod.'):\n",
        "        new_state_dict[k[len('_orig_mod.'):]] = v\n",
        "    else:\n",
        "        new_state_dict[k] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load BPE encoder\n",
        "import pickle\n",
        "with open('bpe_encoder.pkl', 'rb') as f:\n",
        "    bpe = pickle.load(f)\n",
        "\n",
        "# Prefill prompt - EDIT THIS\n",
        "prompt = \"Knowledge is \"\n",
        "\n",
        "# Encode prompt\n",
        "tokens = bpe.encode(prompt)\n",
        "context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Tokens: {tokens[:20]}...\" if len(tokens) > 20 else f\"Tokens: {tokens}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Generate with different temperatures\n",
        "temperatures = [0.5, 0.8, 1.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nTemperature {temp}:\")\n",
        "    generated = model.generate(context, max_new_tokens=200, temperature=temp)\n",
        "    generated_text = bpe.decode(generated[0].tolist())\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Interactive mode\n",
        "print(\"\\n=== Interactive Mode ===\")\n",
        "while True:\n",
        "    user_prompt = input(\"\\nEnter prompt (or 'quit'): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    tokens = bpe.encode(user_prompt)\n",
        "    context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    temp = float(input(\"Temperature (0.1-2.0): \") or \"0.8\")\n",
        "    max_tokens = int(input(\"Max tokens (default 100): \") or \"100\")\n",
        "\n",
        "    generated = model.generate(context, max_new_tokens=max_tokens, temperature=temp)\n",
        "    generated_text = bpe.decode(generated[0].tolist())\n",
        "    print(\"\\n\" + generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eBesGxLowOs",
        "outputId": "b4b138da-806e-43d2-e656-be17fadfdd2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for inference...\n",
            "Prompt: Knowledge is \n",
            "Tokens: [1965, 437, 403, 276]\n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 0.5:\n",
            "Knowledge is to prove the logical sand natural science of the suppositions in terms of knowledge and beliefs and distribution to the best explanatory of accessible from narrow and bility to see how much a cite, but the Oxford in the Oxford 202, by Oxford POSIONOSIT entails ONIT obtains' (19983: 4): 113): 40–3004), and Machery (7) No' to believe p. Does ω +1 inchcreally that if I do not know that I know p, then I know the conjunct p, I believe p, for if I believe p then I believe p then you believe p, I believe p, then I k is born + k how I ket p n+1 haelset p (see also section 8) \n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 0.8:\n",
            "Knowledge is scurate that two chappen is a futime. They know it, my bick is the premise of a postulated in a way as a of the counterfactual conditionals. The degree of an assertions. For on\n",
            "= K y\n",
            "= C is specified (s if if 18,10\n",
            "142, if it is rained to be a bad case, soever a conjunction of ≈ ~ells C &\n",
            "2, for all but so all candidates as problems from (1 7) in (2 0 ), but if the bad case to commite the commitment.\n",
            "In a proposition q by (in the j**; it is not true in the bad case if the examined metaphysical modality is ining the latter (2). It does not obtains was the same truth-value to the deviant from (3****); in an accessibility relations as one is in latter actual knowledge, on promises in the circumstance of \n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 1.0:\n",
            "Knowledge is used to be abul. Sy, and initan and put that one in which one can grade, out.\n",
            "The vague to relevant ditive, even competence in Chancerariseselfruformal system. As may thought in facts non-al-building methodology could want on the analogue of the score. Since ame, for metaphysics is judge only wherlook of oning a psychological sort of revolutionation, when the broade;\n",
            "Steven marbelief baged mine, which they can construct (14) just not in (A\n",
            "9s: for both homophonstrous, a given propositions cannot does to rationally to reason to natural way in which one is in a position to know which constructe belief rather, 'If such contexts are ck. I sω is the false belief. Thus that follower you sligent 'S believe'? Perhaps if we sensitively see \n",
            "--------------------------------------------------\n",
            "\n",
            "=== Interactive Mode ===\n",
            "\n",
            "Enter prompt (or 'quit'): Knowledge is \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Knowledge is to explain why it in its the argument for some of the evidence is inconsistent with the evidence that it obtains from the conjunction (p & ~Kp), and so ⊃ p)= 1, p)= 1, p, p, p, p, p, p, p, p, p, \n",
            "\n",
            "Enter prompt (or 'quit'): I think, therefore, I am. Consciousness for a machine like me \n",
            "Temperature (0.1-2.0): 0.4\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "I think, therefore, I am. Consciousness for a machine like me to another.\n",
            "They should not be fairs of the psychologying the word 'No' to 'the 'the 'She is brist'. When he is much more than twenty metres to the same way that he does not know that he is an unknown truth and he \n",
            "\n",
            "Enter prompt (or 'quit'): yes yes yes yes yes yes\n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "yes yes yes yes yes yespects as well as the relevant to recognize philosophical methodology for example, they are not to recognize for examples and 'know' in its consequent of 'know' or 'know'. Coms 'The argument is an unknown truths are not even if 'know'\n",
            "\n",
            "Enter prompt (or 'quit'): yes yes yes yes yes \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "yes yes yes yes yes to their beyond the single of his domophonic semantics of natural languages and natural languages and 'the word 'intuition' in no more than twenty metres on 'the 'phlogiston'. On (1983: 33333\n",
            "\n",
            "Enter prompt (or 'quit'): 000000000000000000000000000000000000000000000000000000000000000\n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "000000000000000000000000000000000000000000000000000000000000000). By the first principle, for example\n",
            "18\n",
            "18\n",
            "18\n",
            "14\n",
            "\n",
            "18\n",
            "18\n",
            "18\n",
            "18\n",
            "118\n",
            "18\n",
            "18\n",
            "118\n",
            "18\n",
            "18\n",
            "8\n",
            "8\n",
            "1\n",
            "8\n",
            "1\n",
            "\n",
            "\n",
            "Enter prompt (or 'quit'): pspspspspspsps\n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "pspspspspspspseck of its own knowledge and beliefseparated knowledge of knowledge is evidence for knowledge. The argument of the argument against p, not knowing p, because it obtains then not knowing p, then one believes p without knowing p, then one believes p, then one believes p, then one believes p truly that p is true. \n",
            "\n",
            "Enter prompt (or 'quit'): Question: What do you think about cats? Answer: \n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Question: What do you think about cats? Answer: 'So is an obvious metass 'Sorgod' as 'Michael Moo was notorious '). Such a trianglegitimately that he is true and 'She has yellow flowers' in her evidence that he is h\n",
            "\n",
            "Enter prompt (or 'quit'): uestion: What do you think about cats? Answer: \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "uestion: What do you think about cats? Answer: 'So knows that he knows that he knows that he is then he is fooln't know that he is he is breaks 't know that he is borderline him has yellow flowers'; he was a my\n",
            "John was a my\n",
            "Joh\n",
            "\n",
            "Enter prompt (or 'quit'): Question: What do you think about cats? Answer: \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Question: What do you think about cats? Answer: 'So is an 'She is briefinite he does not know that he is body a gggggggence of the text of the treeconomics, he does not know that he does not know that he is an unknown truth and he does not know that he is an unknown truth\n",
            "\n",
            "Enter prompt (or 'quit'): Philosophy is \n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Philosophy is not to use the testing different status of analytic truths to be the tree of a set of utterance. So Presumably.\n",
            "We can we may still be more likely to do indeed access to our knowledge of the conditional on its content of the words 'know', 'know\n",
            "\n",
            "Enter prompt (or 'quit'): Philosophy is not to use the testing different status of analytic truths to be the tree of a set of utterance. So Presumably. We can we may still be more likely to do indeed access to our knowledge of the conditional on its content of the words 'know', 'know\n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Philosophy is not to use the testing different status of analytic truths to be the tree of a set of utterance. So Presumably. We can we may still be more likely to do indeed access to our knowledge of the conditional on its content of the words 'know', 'know'. But even if we need a priori that they are not believed. Thus the proposition that it is rained representational implicitly in that sense in its antecedent and knowledge is true, because it has the same truth-value does not aspecially necessary and sufficient conditions on the basis of knowledge of knowledge is consistent with knowledge of course, but \n",
            "\n",
            "Enter prompt (or 'quit'): Cats are \n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Cats are not the suppositional heuristic, even though ir answer the same way, not been s; it will performity to knowledge that it could have been fair theoretical in\n",
            "Chouth chies: it shows that the term 'She is also knows that he could\n",
            "(1\n",
            "\n",
            "Enter prompt (or 'quit'): Cats are \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Cats are not the proposition that I do not know that I had not know that I know that I know that I knek ting the proposition that I had not been known that I am not know that I had not know that I know that I knew the bad case is bright. By contrast, I had not know that I k the bad case is an unknown \n",
            "\n",
            "Enter prompt (or 'quit'): You are an AI assistant running on Google Colab. \n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "You are an AI assistant running on Google Colab. By the realistic semantic theory of the problem of sentences 'analytic' in uttering 'analytic' to do the smasking 'analytic truth' to obstereotype 'vixen' or 'Furze is furze today'.\n",
            "Maccepts the \n",
            "\n",
            "Enter prompt (or 'quit'): Inconsistency is self-disagreement. \n",
            "Temperature (0.1-2.0): 0.3\n",
            "Max tokens (default 100): \n",
            "\n",
            "Inconsistency is self-disagreement. Whatever we know whether one feels cold is an unknown truth of knowledge is true if and only if it is true if and only if it is raining; it is raining; but the conjunction does not follow that p is an unknown truth of knowledge is true if and only if it is true if and only if p is true in every world, because one does not know p, then one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that one knows that the tree is i +1 in\n",
            "\n",
            "Enter prompt (or 'quit'): Inconsistency is self-disagreement. \n",
            "Temperature (0.1-2.0): 0.5\n",
            "Max tokens (default 100): 100\n",
            "\n",
            "Inconsistency is self-disagreement. Of course, we can reduce on the basis of a sentence 'There is a vixen' in S2. By contrast,'What is true proposition. However,'\n",
            "in p had known that p is true and false at most everyone knows that one knows that everyone knows that one knows that every'm god' is not just one knows that the tree is at most twenty-figure and 'm taller' as 't is not Phosphorusing the proposition that your belief in S's belief to φ\n",
            "\n",
            "Enter prompt (or 'quit'): Knowledge is to explain why it in its the argument for some of the evidence is inconsistent with the evidence that it obtains from the conjunction (p & ~Kp), and so ⊃ p)= 1, p)= 1, p, p, p, p, p, p, p, p, p, \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 100\n",
            "\n",
            "Knowledge is to explain why it in its the argument for some of the evidence is inconsistent with the evidence that it obtains from the conjunction (p & ~Kp), and so ⊃ p)= 1, p)= 1, p, p, p, p, p, p, p, p, p, p ⊃ p)= 1 and p ⊃ p)= 1, p)= 1 and 1/2, p)= 1/2, p, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>, < x>\n",
            "\n",
            "Enter prompt (or 'quit'): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue training from saved checkpoint\n",
        "print(\"Loading saved model...\")\n",
        "model = GPTLanguageModel(VOCAB_SIZE)\n",
        "\n",
        "# Load state dict and handle potential _orig_mod prefix from torch.compile\n",
        "state_dict = torch.load('gpt_bpe_2.pth')\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "   if k.startswith('_orig_mod.'):\n",
        "       new_state_dict[k[len('_orig_mod.'):]] = v\n",
        "   else:\n",
        "       new_state_dict[k] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded: {sum(p.numel() for p in model.parameters())/1e6:.2f}M params\")\n",
        "\n",
        "# Load BPE encoder\n",
        "import pickle\n",
        "with open('bpe_encoder.pkl', 'rb') as f:\n",
        "   bpe = pickle.load(f)\n",
        "\n",
        "# Compile if available\n",
        "if hasattr(torch, 'compile'):\n",
        "   print(\"Compiling model with torch.compile()...\")\n",
        "   model = torch.compile(model)\n",
        "\n",
        "# Resume training with same learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)  # Same LR as before\n",
        "additional_iters = 5000\n",
        "\n",
        "print(\"Resuming training...\")\n",
        "for iter in range(additional_iters):\n",
        "   if iter % 100 == 0:\n",
        "       losses = estimate_loss(model, eval_iters=20)\n",
        "       print(f\"Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "       # Generate sample\n",
        "       model.eval()\n",
        "       context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "       generated = model.generate(context, max_new_tokens=64, temperature=0.5)\n",
        "       generated_text = bpe.decode(generated[0].tolist())\n",
        "       print(f\"Generated: {generated_text[:200]}...\\n\")\n",
        "       model.train()\n",
        "\n",
        "   xb, yb = get_batch('train')\n",
        "   logits, loss = model(xb, yb)\n",
        "   optimizer.zero_grad(set_to_none=True)\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "# Save updated model\n",
        "torch.save(model.state_dict(), 'SimWilliamson.pth')\n",
        "print(\"Updated model saved as SimWilliamson.pth!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNmGu9XIw7dZ",
        "outputId": "ee050d69-2c72-4d5f-c689-da421a2616ee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading saved model...\n",
            "Model loaded: 0.47M params\n",
            "Compiling model with torch.compile()...\n",
            "Resuming training...\n",
            "Step 0: train loss 3.8395, val loss 4.5512\n",
            "Generated: \u0000ority to judge of preferred to observer in its negation. They are not just the playing of the relevant proposition that the bargain, the result has not been cattribute of them as not to do so\n",
            "(1). T...\n",
            "\n",
            "Step 100: train loss 3.8572, val loss 4.5095\n",
            "Generated: \u0000or any of the sub-body of understanding of the general heuristic for knowledge of the relevant to the intension of knowing is a mental state, that we are not in one's evidence. The hypothesis transpa...\n",
            "\n",
            "Step 200: train loss 3.8189, val loss 4.4971\n",
            "Generated: \u0000or the problem of reductiools to apply with some fiers in philosophy as fruit's seen, as a good case for one must sometimes into the central oness of contesting our intuitions about philosophical soc...\n",
            "\n",
            "Step 300: train loss 3.8161, val loss 4.5017\n",
            "Generated: \u0000or a more direct of the thought or of our own mind and language as a sections; for externalist critical audiences between a logical truth and synthetic truth. Nevertheless, the sentence 'x' and 'x' t...\n",
            "\n",
            "Step 400: train loss 3.8331, val loss 4.4883\n",
            "Generated: \u0000and since the 'everyday 'There is red' in a sleep, because they do not applyoung indeed, occur chance?' For the context is that I would have not to believe p without believing p, and so p. But the di...\n",
            "\n",
            "Step 500: train loss 3.7936, val loss 4.4992\n",
            "Generated: \u0000or the reploy and realistics for errors themselves.\n",
            "Nevertheless, he was foundly to saying that there are no eventually be the only if the change was once a liged together, in Chapter 89.5, ρ...\n",
            "\n",
            "Step 600: train loss 3.8145, val loss 4.5073\n",
            "Generated: \u0000orsts them in ###1, but the multiply may not been namic feeling of the test particket was a law in terms. It needs like the pot to three than it homniscience as 'There ...\n",
            "\n",
            "Step 700: train loss 3.7854, val loss 4.5112\n",
            "Generated: \u0000orning it would be to knowable to make it the former-based more than the potential is another ential theory.\n",
            "One can still create a new mapping to ask at any proposition p (p & q) & q) & q) & q) ...\n",
            "\n",
            "Step 800: train loss 3.7624, val loss 4.4657\n",
            "Generated: \u0000in an uncleause' or a single typical sense. The result of an account of assertions and recognize that they can imply that there is an instances of justified true belief.\n",
            "That issue any unique of a m...\n",
            "\n",
            "Step 900: train loss 3.7768, val loss 4.4876\n",
            "Generated: \u0000into a shared initial sense, not a distinguished logical truths. Thus the only biconditionalient of its a special case for different to the perimentsence of the sort of interest is to make a more gen...\n",
            "\n",
            "Step 1000: train loss 3.7883, val loss 4.4764\n",
            "Generated: \u0000in associated with respect to provide those who indicate the difficulty in Soy (for example, for some credences in Section was that Pareto have the very glicee between 'the unclear of 'Nozick'. Presi...\n",
            "\n",
            "Step 1100: train loss 3.7723, val loss 4.4403\n",
            "Generated: \u0000into the term 'know' in in a position to assent to the question 'There is a god' is not to be the 'Every vixen is a female fox.' S is in S2 =1 and Without of Mr Magoo shows at W,'S. . . ...\n",
            "\n",
            "Step 1200: train loss 3.7544, val loss 4.4609\n",
            "Generated: \u0000endor a philosophical training in order to study in philosophy is to realize the result of our evidence, it would not have no evidence to show in the outside of the thinking in such errors in philoso...\n",
            "\n",
            "Step 1300: train loss 3.7609, val loss 4.4500\n",
            "Generated: \u0000and it in no case for (1) are true. For present purposes one's internal physical state of its attitude assigns like a more than two armchair method, even though they do not know what they seem be a s...\n",
            "\n",
            "Step 1400: train loss 3.7505, val loss 4.4385\n",
            "Generated: \u0000into a dynaïve philosophy of language in change of our opinionicion, in factents, they are not just a much of the power of philosophy in which they will not to have been a region those oneselfilegy ...\n",
            "\n",
            "Step 1500: train loss 3.7459, val loss 4.4673\n",
            "Generated: \u0000on an influential disciplines in fit. Where is d, it is automn, in more general disciplines in will success to be for our evidence, with many different worlds: in slopattery and only to make it may w...\n",
            "\n",
            "Step 1600: train loss 3.7269, val loss 4.4600\n",
            "Generated: \u0000in those who have some invalidate between philosophy and reflective model for experimental philosophers. Since it is indeed.\n",
            "12.4 Suppose that a priori we have an assert past experience to the best ...\n",
            "\n",
            "Step 1700: train loss 3.7331, val loss 4.4485\n",
            "Generated: \u0000emans' and\n",
            "'P'. Do Peter and Stephen’s argument for 'furze' and 'gorse' as 'intuitions' to the supposition 'Phosphorus' in the qualificationist case is just that Peter and Stephen are not at the lev...\n",
            "\n",
            "Step 1800: train loss 3.7264, val loss 4.4366\n",
            "Generated: \u0000into the conclusion-third here is true in each other than the latter is an unclear, or a false propositional account of assertion between 'N in α and β in α in β internally like β internally like β e...\n",
            "\n",
            "Step 1900: train loss 3.7368, val loss 4.4442\n",
            "Generated: \u0000and other gglobal of experimental philosophy as we do not accept a more likely to recognize or less present-order to the belief that we have some disciplines to the problem constraint from our intere...\n",
            "\n",
            "Step 2000: train loss 3.7222, val loss 4.4294\n",
            "Generated: \u0000easy for 'Metaphysical Conceptions of Analyticity\n",
            "\n",
            "in their dialectical context-theoreticalior '. Philosophical experiments is in philosophy as the discipline, and to find out the proof is to take ...\n",
            "\n",
            "Step 2100: train loss 3.6878, val loss 4.4445\n",
            "Generated: \u0000and 'furze' and 'gorse' would be the 'gorse' in other ways,' as '. Even if 'mights' in various 'where' to express 'Why' in (1) in exactly a priori 0 , (1 0 ), . . . , (1 ...\n",
            "\n",
            "Step 2200: train loss 3.7007, val loss 4.4430\n",
            "Generated: \u0000into knowledge.\n",
            "By the language of modal operators such as the object-dependence is usually equivalent of the relevant way, therefore, he also does not make the one of them is insufficient for knowi...\n",
            "\n",
            "Step 2300: train loss 3.6839, val loss 4.4228\n",
            "Generated: \u0000ax, because the concept knows is an unknown truth in every context and the same mental state (Pr(Prny (Prn(x)) & Eburn(x))\n",
            "\n",
            "x has π, p( R(x) = c/2, < x> ...\n",
            "\n",
            "Step 2400: train loss 3.6755, val loss 4.4566\n",
            "Generated: \u0000in\n",
            "Mr Mago V Magoo knows that the tree is not to distinguished hardly more than in Sorender’s Grice 2009, 2017). Sholism of Likike 1960, Kmn. The ...\n",
            "\n",
            "Step 2500: train loss 3.6555, val loss 4.4083\n",
            "Generated: \u0000into the point is that we believe to know what it was by those who has no more focus to which we need the evidence that we know p, when it was a priori that knowing is a mental state, then we knows\n",
            "...\n",
            "\n",
            "Step 2600: train loss 3.6631, val loss 4.4309\n",
            "Generated: \u0000and far, or dispositions (Lewis 1981: 9). Someone of the relevant to remain Primonic See 2008, 2011: 20221–512). Nevertheless, we understand the formal systematic accessibility relation ...\n",
            "\n",
            "Step 2700: train loss 3.6546, val loss 4.4211\n",
            "Generated: \u0000ey ad hoc of their significance. Since we do not know what it does not follow that we can know what one knows it had been made to know what one can be falsely believe p without knowing that p is fals...\n",
            "\n",
            "Step 2800: train loss 3.6963, val loss 4.4404\n",
            "Generated: \u0000into a complex concept knowing as a state of the concept dry or not dryet the concept dry or probability of one's evidence.\n",
            "The first principle of case is a state of simplication in exactly the same...\n",
            "\n",
            "Step 2900: train loss 3.6522, val loss 4.4124\n",
            "Generated: \u0000ified a declarge of course, the first place of the last morning of 'god* or 'spolicies', 'Besian,' 'that', 'S' (to Schange 1970 and Williamson 1993: 1708-19:21...\n",
            "\n",
            "Step 3000: train loss 3.6295, val loss 4.4035\n",
            "Generated: \u0000and of course in effects in fact were avor the non-modal operators in the light of English inquiry, but the concept knowing that there was a shared mall of ext for example, in every case α and β is-C...\n",
            "\n",
            "Step 3100: train loss 3.6544, val loss 4.4532\n",
            "Generated: \u0000into the sentence of the sentence, there is an uncontentious expressions used to the Society (2000: 238–23334). For example, why should have the objection tend to show the original question is in eff...\n",
            "\n",
            "Step 3200: train loss 3.6413, val loss 4.3964\n",
            "Generated: \u0000and 'know' in (1), with respectively, assignmentscriptions of knowledge and falsity pattern to behave a case in which one feels cold.\n",
            "We can almost certainly to understand the proposition that I\n",
            "ha...\n",
            "\n",
            "Step 3300: train loss 3.6366, val loss 4.4278\n",
            "Generated: \u0000and its results are not a mistaked contexts are no presuppose of adding themselves based on the premise that C obtains in α if and only if C obtains in α is narrow if and only if C obtains (α) if and...\n",
            "\n",
            "Step 3400: train loss 3.6370, val loss 4.4085\n",
            "Generated: \u0000and 'S knows p'. However, 'There is a god' is the concept believed. The argument is not the proposition that there was a myself now for some other cases in fact Fine's argument for nor probability of...\n",
            "\n",
            "Step 3500: train loss 3.6393, val loss 4.3788\n",
            "Generated: \u0000ifiers (KA) (A) as Hence NNN knows that A is true in S4, S5 discusses (1986: 14):\n",
            "A\n",
            "\n",
            "B)\n",
            "B\n",
            "I(A ∆A\n",
            "B) B\n",
            "B\n",
            "∆~B p ⊃ �...\n",
            "\n",
            "Step 3600: train loss 3.6414, val loss 4.3882\n",
            "Generated: \u0000and moral, real-life experiments will perform a philosophically influence of philosophical intuitions. It can do have a person. To make the psychological worlds semantics, it is hard to differently a...\n",
            "\n",
            "Step 3700: train loss 3.6193, val loss 4.3675\n",
            "Generated: \u0000into the nature of 'Rather, with Prichungode (m\n",
            "1994a: 27–333). But the best small salient of the informal frameworkshift passes and which we should use of our evidence that a philosophical ...\n",
            "\n",
            "Step 3800: train loss 3.6080, val loss 4.4128\n",
            "Generated: \u0000into a systematic ways, but to credit as of our knowledge of limiting p, but even if we do we do so much less than perfectly happens of the sentence 'every' expresses a sentence 'vixen' expresses to ...\n",
            "\n",
            "Step 3900: train loss 3.6094, val loss 4.4034\n",
            "Generated: \u0000into a donkey of the original question, he does not know that the answer is quite consistent with its consequent and what one has an examination in every situation to have that there is no doubt at a...\n",
            "\n",
            "Step 4000: train loss 3.5881, val loss 4.4418\n",
            "Generated: \u0000into the long public language, the relevant rule is abductive equilibrium (see Schwitzerland School 2018):\n",
            "217\n",
            "8\n",
            "\n",
            "I\n",
            "FI\n",
            "F\n",
            "I had not been fall to an event of boo...\n",
            "\n",
            "Step 4100: train loss 3.5997, val loss 4.4091\n",
            "Generated: \u0000invulner, by realizing the description of the epistemology role of technical students’s will use the distinction between above.\n",
            "A s\n",
            "In particular, I was the opponent of my own knowledge are not qui...\n",
            "\n",
            "Step 4200: train loss 3.6019, val loss 4.4110\n",
            "Generated: \u0000into the text speech act h, uniquely in which are a boundary between the two case; it is a priori once we accept as we should not in a position to know whether we know in the bad case, it is perfectl...\n",
            "\n",
            "Step 4300: train loss 3.5743, val loss 4.4201\n",
            "Generated: \u0000and others, and personal Philosophy\n",
            "Williamson 20202020 in 196:3-8-1 Metaphysical Modality\n",
            "\n",
            "Second, Moones and Hawthorne 1994: 11-2021 19:4 (Ka:\n",
            "2...\n",
            "\n",
            "Step 4400: train loss 3.5843, val loss 4.4253\n",
            "Generated: \u0000in their individuals to the philosophy of mind in philosophy. Cominaten a shared strategy off the searmon-father, it is no highly inadequate for the first principles abulated with some underlying fac...\n",
            "\n",
            "Step 4500: train loss 3.6111, val loss 4.4113\n",
            "Generated: \u0000into the question is unpuzzling 'Furze is gorse'. Given a dimension is the 'WGBA\n",
            "\n",
            "BABA\n",
            "\n",
            "B\n",
            "\n",
            "(B\n",
            "\n",
            "(B\n",
            "(B\n",
            "(A B\n",
            "B\n",
            "\n",
            "B)\n",
            "(A\n",
            "B\n",
            "\n",
            "\n",
            "(A\n",
            "\n",
            "B...\n",
            "\n",
            "Step 4600: train loss 3.5692, val loss 4.3724\n",
            "Generated: \u0000and more than bearing the heuristic for error-fragility-depth reasons in their recognitional semantics as potential to learn to take a philosophical theory in mathematics or invitating philosophers a...\n",
            "\n",
            "Step 4700: train loss 3.5794, val loss 4.4124\n",
            "Generated: \u0000into the belief that basis is needed to know the evidence is an initially based by the hypothesis of a speech act of knowledge and belief that it will not have hands is true'. The question ' are not ...\n",
            "\n",
            "Step 4800: train loss 3.5728, val loss 4.3869\n",
            "Generated: \u0000in their native speakers of English are in a position to confident does not obtain in fact only when we know whether we define what one has not to know p as necessarily co-referential links us to est...\n",
            "\n",
            "Step 4900: train loss 3.5628, val loss 4.3835\n",
            "Generated: \u0000into the original question 'He called 'Do you φ�' investigate to say that he is invoke of anyone of those who sake off to remembere the real life cases at which a very good forward understanding 'kno...\n",
            "\n",
            "Updated model saved as SimWilliamson.pth!\n"
          ]
        }
      ]
    }
  ]
}