{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaJi7BG10Jl99r7eqijXsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oak-hu/oak-hu.github.io/blob/dependabot%2Fgithub_actions%2Fci-dependencies-cb3525d1d8/sim_williamson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f523ec2",
        "outputId": "1fdf216c-c581-4c3d-dbf1-8c8733d00315"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download text\n",
        "url = \"https://oak-hu.github.io/Introduction.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "print(f\"Downloaded {len(text):,} characters\")\n",
        "\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = 2048  # Target vocab size for BPE\n",
        "batch_size = 64\n",
        "block_size = 64\n",
        "max_iters = 10000\n",
        "eval_interval = 250\n",
        "learning_rate = 1e-4\n",
        "n_embd = 128\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.2\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class BytePairEncoder:\n",
        "    def __init__(self, vocab_size=2048):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.byte_to_token = {}\n",
        "        self.token_to_byte = {}\n",
        "        self.merges = []\n",
        "\n",
        "    def get_stats(self, ids):\n",
        "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
        "        counts = Counter()\n",
        "        for pair in zip(ids, ids[1:]):\n",
        "            counts[pair] += 1\n",
        "        return counts\n",
        "\n",
        "    def merge(self, ids, pair, idx):\n",
        "        \"\"\"Merge all occurrences of pair into idx\"\"\"\n",
        "        newids = []\n",
        "        i = 0\n",
        "        while i < len(ids):\n",
        "            if i < len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n",
        "                newids.append(idx)\n",
        "                i += 2\n",
        "            else:\n",
        "                newids.append(ids[i])\n",
        "                i += 1\n",
        "        return newids\n",
        "\n",
        "    def train(self, text, verbose=True):\n",
        "        \"\"\"Train BPE on text\"\"\"\n",
        "        # Start with byte-level tokens\n",
        "        tokens = list(text.encode('utf-8'))\n",
        "\n",
        "        # Initialize vocab with single bytes\n",
        "        for i in range(256):\n",
        "            self.byte_to_token[i] = i\n",
        "            self.token_to_byte[i] = bytes([i])\n",
        "\n",
        "        num_merges = self.vocab_size - 256\n",
        "        ids = list(tokens)\n",
        "\n",
        "        for i in tqdm(range(num_merges), desc=\"Training BPE\"):\n",
        "            stats = self.get_stats(ids)\n",
        "            if not stats:\n",
        "                break\n",
        "\n",
        "            # Find most frequent pair\n",
        "            pair = max(stats, key=stats.get)\n",
        "            idx = 256 + i\n",
        "\n",
        "            # Perform merge\n",
        "            ids = self.merge(ids, pair, idx)\n",
        "            self.merges.append((pair, idx))\n",
        "\n",
        "            # Update token mappings\n",
        "            self.token_to_byte[idx] = self.token_to_byte[pair[0]] + self.token_to_byte[pair[1]]\n",
        "\n",
        "            if verbose and i % 100 == 0:\n",
        "                compression_ratio = len(tokens) / len(ids)\n",
        "                print(f\"Merge {i}: {pair} -> {idx}, compression: {compression_ratio:.2f}x\")\n",
        "\n",
        "        # Build reverse mapping\n",
        "        self.byte_to_token = {v: k for k, v in self.token_to_byte.items() if k < self.vocab_size}\n",
        "\n",
        "        print(f\"Final vocab size: {len(self.token_to_byte)}\")\n",
        "        print(f\"Compression ratio: {len(tokens) / len(ids):.2f}x\")\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to tokens\"\"\"\n",
        "        tokens = list(text.encode('utf-8'))\n",
        "\n",
        "        # Apply merges in order\n",
        "        for pair, idx in self.merges:\n",
        "            tokens = self.merge(tokens, pair, idx)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        \"\"\"Decode tokens back to text\"\"\"\n",
        "        byte_list = []\n",
        "        for token in tokens:\n",
        "            if token in self.token_to_byte:\n",
        "                byte_list.extend(self.token_to_byte[token])\n",
        "\n",
        "        return bytes(byte_list).decode('utf-8', errors='replace')\n",
        "\n",
        "# Data loading function\n",
        "def get_batch():\n",
        "   # Get batch from the full training data\n",
        "   ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "   x = torch.stack([train_data[i:i+block_size] for i in ix])\n",
        "   y = torch.stack([train_data[i+1:i+block_size+1] for i in ix])\n",
        "   x, y = x.to(device), y.to(device)\n",
        "   return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=50):\n",
        "   # Evaluate loss on the full training data (no validation split)\n",
        "   out = {}\n",
        "   model.eval()\n",
        "   losses = torch.zeros(eval_iters)\n",
        "   for k in range(eval_iters):\n",
        "       X, Y = get_batch()\n",
        "       logits, loss = model(X, Y)\n",
        "       losses[k] = loss.item()\n",
        "   out['train'] = losses.mean()\n",
        "   out['val'] = losses.mean() # Still report 'val' for consistency, using train data\n",
        "   model.train()\n",
        "   return out\n",
        "\n",
        "# Model components - simplified\n",
        "class Head(nn.Module):\n",
        "   def __init__(self, head_size):\n",
        "       super().__init__()\n",
        "       self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "       self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "   def forward(self, x):\n",
        "       B,T,C = x.shape\n",
        "       k = self.key(x)\n",
        "       q = self.query(x)\n",
        "       wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "       wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "       wei = F.softmax(wei, dim=-1)\n",
        "       v = self.value(x)\n",
        "       out = wei @ v\n",
        "       return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "   def __init__(self, num_heads, head_size):\n",
        "       super().__init__()\n",
        "       self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "       self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "   def forward(self, x):\n",
        "       out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "       out = self.proj(out)\n",
        "       return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "   def __init__(self, n_embd):\n",
        "       super().__init__()\n",
        "       self.net = nn.Sequential(\n",
        "           nn.Linear(n_embd, 4 * n_embd),\n",
        "           nn.ReLU(),  # ReLU instead of GELU\n",
        "           nn.Linear(4 * n_embd, n_embd),\n",
        "       )\n",
        "\n",
        "   def forward(self, x):\n",
        "       return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "   def __init__(self, n_embd, n_head):\n",
        "       super().__init__()\n",
        "       head_size = n_embd // n_head\n",
        "       self.sa = MultiHeadAttention(n_head, head_size)\n",
        "       self.ffwd = FeedForward(n_embd)\n",
        "       self.ln1 = nn.LayerNorm(n_embd)\n",
        "       self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = x + self.sa(self.ln1(x))\n",
        "       x = x + self.ffwd(self.ln2(x))\n",
        "       return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "   def __init__(self, vocab_size):\n",
        "       super().__init__()\n",
        "       self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "       self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "       self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "       self.ln_f = nn.LayerNorm(n_embd)\n",
        "       self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "   def forward(self, idx, targets=None):\n",
        "       B, T = idx.shape\n",
        "       tok_emb = self.token_embedding_table(idx)\n",
        "       pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "       x = tok_emb + pos_emb\n",
        "       x = self.blocks(x)\n",
        "       x = self.ln_f(x)\n",
        "       logits = self.lm_head(x)\n",
        "\n",
        "       if targets is None:\n",
        "           loss = None\n",
        "       else:\n",
        "           B, T, C = logits.shape\n",
        "           logits = logits.view(B*T, C)\n",
        "           targets = targets.view(B*T)\n",
        "           loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "       return logits, loss\n",
        "\n",
        "   @torch.no_grad()\n",
        "   def generate(self, idx, max_new_tokens, temperature=1.0):\n",
        "       for _ in range(max_new_tokens):\n",
        "           idx_cond = idx[:, -block_size:]\n",
        "           logits, _ = self(idx_cond)\n",
        "           logits = logits[:, -1, :] / temperature\n",
        "           probs = F.softmax(logits, dim=-1)\n",
        "           idx_next = torch.multinomial(probs, num_samples=1)\n",
        "           idx = torch.cat((idx, idx_next), dim=1)\n",
        "       return idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd9qN8FriPZP",
        "outputId": "ecd3beda-04b3-45b1-bead-8bc0e2968294"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Downloaded 2,804,543 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train BPE\n",
        "print(\"Training BPE encoder...\")\n",
        "bpe = BytePairEncoder(vocab_size=VOCAB_SIZE)\n",
        "bpe.train(text, verbose=True)\n",
        "\n",
        "# Test encoder/decoder\n",
        "test_strings = [\n",
        "    \"Hello world!\",\n",
        "    \"  Multiple   spaces  \",\n",
        "    \"New\\nlines\\nwork\",\n",
        "    \"Special chars: !@#$%^&*()\",\n",
        "    \"Numbers: 12345\",\n",
        "    \"Mixed: Hello 123 world!\\n\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Encoder/Decoder Test ---\")\n",
        "for test_str in test_strings:\n",
        "    encoded = bpe.encode(test_str)\n",
        "    decoded = bpe.decode(encoded)\n",
        "    match = \"✓\" if test_str == decoded else \"✗\"\n",
        "    print(f\"{match} '{test_str}' -> {len(encoded)} tokens -> '{decoded}'\")\n",
        "\n",
        "# Save the bpe object\n",
        "output_path = '/content/drive/MyDrive/williamson_bpe.pkl'\n",
        "with open(output_path, 'wb') as f:\n",
        "    pickle.dump(bpe, f)\n",
        "\n",
        "print(f\"BPE encoder saved to {output_path}\")"
      ],
      "metadata": {
        "id": "HSPf8eCLi6sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln-9ueRlI24z",
        "outputId": "d0d0d67a-e7a9-40ac-b8a0-1473e19e9d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 2,804,543 characters...\n",
            "Checkpoint 100 chars: 2,453 tokens, compression: 0.04x, time: 1.6s\n",
            "Checkpoint 1,000 chars: 2,453 tokens, compression: 0.41x, time: 1.6s\n",
            "Checkpoint 10,000 chars: 2,453 tokens, compression: 4.08x, time: 1.6s\n",
            "Checkpoint 100,000 chars: 24,200 tokens, compression: 4.13x, time: 18.6s\n",
            "Checkpoint 100,000 chars: 24,200 tokens, compression: 4.13x, time: 18.6s\n",
            "Checkpoint 200,000 chars: 49,897 tokens, compression: 4.01x, time: 38.8s\n",
            "Checkpoint 300,000 chars: 75,706 tokens, compression: 3.96x, time: 57.3s\n",
            "Checkpoint 400,000 chars: 102,988 tokens, compression: 3.88x, time: 77.8s\n",
            "Checkpoint 500,000 chars: 127,903 tokens, compression: 3.91x, time: 97.1s\n",
            "Checkpoint 600,000 chars: 154,788 tokens, compression: 3.88x, time: 116.0s\n",
            "Checkpoint 700,000 chars: 180,873 tokens, compression: 3.87x, time: 136.4s\n",
            "Checkpoint 800,000 chars: 212,471 tokens, compression: 3.77x, time: 157.9s\n",
            "Checkpoint 900,000 chars: 240,353 tokens, compression: 3.74x, time: 179.1s\n",
            "Checkpoint 1,000,000 chars: 267,031 tokens, compression: 3.74x, time: 198.3s\n",
            "Checkpoint 1,100,000 chars: 293,624 tokens, compression: 3.75x, time: 219.2s\n",
            "Checkpoint 1,200,000 chars: 322,652 tokens, compression: 3.72x, time: 239.4s\n",
            "Checkpoint 1,300,000 chars: 350,304 tokens, compression: 3.71x, time: 260.6s\n",
            "Checkpoint 1,400,000 chars: 377,314 tokens, compression: 3.71x, time: 279.8s\n",
            "Checkpoint 1,500,000 chars: 405,304 tokens, compression: 3.70x, time: 301.1s\n",
            "Checkpoint 1,600,000 chars: 431,445 tokens, compression: 3.71x, time: 319.9s\n",
            "Checkpoint 1,700,000 chars: 458,607 tokens, compression: 3.71x, time: 339.9s\n",
            "Checkpoint 1,800,000 chars: 484,854 tokens, compression: 3.71x, time: 359.9s\n",
            "Checkpoint 1,900,000 chars: 510,903 tokens, compression: 3.72x, time: 378.8s\n",
            "Checkpoint 2,000,000 chars: 536,137 tokens, compression: 3.73x, time: 399.0s\n",
            "Checkpoint 2,100,000 chars: 562,416 tokens, compression: 3.73x, time: 417.7s\n",
            "Checkpoint 2,200,000 chars: 592,032 tokens, compression: 3.72x, time: 439.9s\n",
            "Checkpoint 2,300,000 chars: 619,273 tokens, compression: 3.71x, time: 459.5s\n",
            "Checkpoint 2,400,000 chars: 646,134 tokens, compression: 3.71x, time: 482.0s\n",
            "Checkpoint 2,500,000 chars: 672,425 tokens, compression: 3.72x, time: 502.1s\n",
            "Checkpoint 2,600,000 chars: 699,635 tokens, compression: 3.72x, time: 528.2s\n",
            "Checkpoint 2,700,000 chars: 726,536 tokens, compression: 3.72x, time: 555.8s\n",
            "Checkpoint 2,800,000 chars: 754,286 tokens, compression: 3.71x, time: 579.0s\n",
            "\n",
            "Encoding complete: 755,587 tokens in 580.7s\n",
            "Final compression ratio: 3.71x\n",
            "\n",
            "Train tokens: 680,028\n",
            "Val tokens: 75,559\n",
            "Saved encoder to bpe_encoder.pkl\n"
          ]
        }
      ],
      "source": [
        "def encode_with_checkpoints(bpe, text):\n",
        "    \"\"\"Efficiently encode text with progress checkpoints\"\"\"\n",
        "\n",
        "    # Optimized encoding using chunking\n",
        "    chunk_size = 10000\n",
        "    encoded_chunks = []\n",
        "    total_chars = len(text)\n",
        "\n",
        "    checkpoints = [100, 1000, 10000, 100000]\n",
        "    next_checkpoint_idx = 0\n",
        "    chars_processed = 0\n",
        "\n",
        "    print(f\"Encoding {total_chars:,} characters...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(0, total_chars, chunk_size):\n",
        "        chunk = text[i:i+chunk_size]\n",
        "        encoded_chunk = bpe.encode(chunk)\n",
        "        encoded_chunks.extend(encoded_chunk)\n",
        "\n",
        "        chars_processed += len(chunk)\n",
        "\n",
        "        # Check for checkpoints\n",
        "        while next_checkpoint_idx < len(checkpoints) and chars_processed >= checkpoints[next_checkpoint_idx]:\n",
        "            cp = checkpoints[next_checkpoint_idx]\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_so_far = len(encoded_chunks)\n",
        "            compression = cp / tokens_so_far if tokens_so_far > 0 else 0\n",
        "            print(f\"Checkpoint {cp:,} chars: {tokens_so_far:,} tokens, \"\n",
        "                  f\"compression: {compression:.2f}x, time: {elapsed:.1f}s\")\n",
        "            next_checkpoint_idx += 1\n",
        "\n",
        "        # After 100k, checkpoint every 100k\n",
        "        if chars_processed >= 100000 and chars_processed % 100000 == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            tokens_so_far = len(encoded_chunks)\n",
        "            compression = chars_processed / tokens_so_far if tokens_so_far > 0 else 0\n",
        "            print(f\"Checkpoint {chars_processed:,} chars: {tokens_so_far:,} tokens, \"\n",
        "                  f\"compression: {compression:.2f}x, time: {elapsed:.1f}s\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nEncoding complete: {len(encoded_chunks):,} tokens in {total_time:.1f}s\")\n",
        "    print(f\"Final compression ratio: {total_chars / len(encoded_chunks):.2f}x\")\n",
        "\n",
        "    return encoded_chunks\n",
        "\n",
        "# Encode the full text\n",
        "encoded_text = encode_with_checkpoints(bpe, text)\n",
        "\n",
        "# Convert to tensor and prepare data splits\n",
        "data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"\\nTrain tokens: {len(train_data):,}\")\n",
        "print(f\"Val tokens: {len(val_data):,}\")\n",
        "\n",
        "# Save encoder and data for future use\n",
        "with open('bpe_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(bpe, f)\n",
        "print(\"Saved encoder to bpe_encoder.pkl\")\n",
        "\n",
        "# Save the encoded text\n",
        "encoded_output_path = '/content/drive/MyDrive/williamson_encoded.pkl'\n",
        "with open(encoded_output_path, 'wb') as f:\n",
        "    pickle.dump(encoded_text, f)\n",
        "\n",
        "print(f\"Encoded text saved to {encoded_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load encoded text and BPE encoder from Drive\n",
        "encoded_text = None\n",
        "bpe = None\n",
        "train_data = None # Initialize train_data here\n",
        "\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/williamson_encoded.pkl', 'rb') as f:\n",
        "        encoded_text = pickle.load(f)\n",
        "    print(\"Loaded encoded text from Google Drive.\")\n",
        "    with open('/content/drive/MyDrive/williamson_bpe.pkl', 'rb') as f:\n",
        "        bpe = pickle.load(f)\n",
        "    print(\"Loaded BPE encoder from Google Drive.\")\n",
        "\n",
        "    # Assign the full data to train_data\n",
        "    train_data = torch.tensor(encoded_text, dtype=torch.long)\n",
        "    print(f\"\\nTrain tokens (full dataset): {len(train_data):,}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Encoded text or BPE encoder not found in Google Drive.\")\n",
        "    print(\"Please ensure 'williamson_encoded.pkl' and 'williamson_bpe.pkl' are in '/content/drive/MyDrive/'.\")\n",
        "\n",
        "\n",
        "# Only proceed with training if data is loaded successfully\n",
        "if train_data is not None and bpe is not None:\n",
        "    # Initialize model\n",
        "    model = GPTLanguageModel(VOCAB_SIZE)\n",
        "    model = model.to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "    # Training with compile if available\n",
        "    if hasattr(torch, 'compile'):\n",
        "       print(\"Compiling model with torch.compile()...\")\n",
        "       model = torch.compile(model)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    for iter in range(max_iters):\n",
        "\n",
        "       # Evaluation and generation at checkpoints\n",
        "       if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "           losses = estimate_loss(model, eval_iters=20)  # Fewer eval iters\n",
        "           # Report both train and val losses as train loss (no separate val data)\n",
        "           print(f\"\\nStep {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "           # Generate sample\n",
        "           model.eval()\n",
        "           context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "           generated = model.generate(context, max_new_tokens=64, temperature=0.5)\n",
        "           generated_text = bpe.decode(generated[0].tolist())\n",
        "           print(f\"Generated (64 tokens):\\n{generated_text}\\n\")\n",
        "           model.train()\n",
        "\n",
        "       # Training step\n",
        "       xb, yb = get_batch() # Call get_batch without split argument\n",
        "       logits, loss = model(xb, yb)\n",
        "       optimizer.zero_grad(set_to_none=True)\n",
        "       loss.backward()\n",
        "       optimizer.step()\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "    # Final generation\n",
        "    model.eval()\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    generated = model.generate(context, max_new_tokens=64, temperature=0.5)\n",
        "    final_text = bpe.decode(generated[0].tolist())\n",
        "    print(f\"\\nFinal generation (500 tokens):\\n{final_text}\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'sim_williamson.pth')\n",
        "    print(\"\\nModel saved to sim_williamson.pth\")\n",
        "else:\n",
        "    print(\"\\nSkipping model training due to data loading error.\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/sim_williamson_1e4.pth')\n",
        "print(\"\\nModel saved to /content/drive/MyDrive/sim_williamson_1e4.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atCiJqCjks1z",
        "outputId": "5a243096-a313-4c44-b5aa-cd0d8a2bc6c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded encoded text from Google Drive.\n",
            "Loaded BPE encoder from Google Drive.\n",
            "\n",
            "Train tokens (full dataset): 755,587\n",
            "Model parameters: 1.33M\n",
            "Compiling model with torch.compile()...\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Step 0: train loss 7.7949, val loss 7.7949\n",
            "Generated (64 tokens):\n",
            "\u0000obprobabilbeliefs construcknowledgIf withus, . S. Away  i ationown basFregeit ese tiesconditionquirge ilosophences intensioncorreccomplex socistricawForright y to mathematoo best logicilicauscharactercesfailbackus, depends ll ainStion of s. The |furzuch curareoriginal section general prcounterfactualpractic. It is  appro\n",
            "\n",
            "\n",
            "Step 250: train loss 6.9902, val loss 6.9902\n",
            "Generated (64 tokens):\n",
            "\u0000inpr'. snt as . oto ces st erone ms aal, 'its err of teiflbchicalmone )vs 's is , t' more e to yMt or it , itsiis that ely iinsge \n",
            "\n",
            "\n",
            "Step 500: train loss 6.9484, val loss 6.9484\n",
            "Generated (64 tokens):\n",
            "\u0000a of in oning speC( rbasnot 'and -regun the ytonfor es (for w to es y-is to the innto be ysin the preal ung that posiformrea insitin ers ededinsa al d.\n",
            "lyhave w\n",
            "\n",
            "\n",
            "Step 750: train loss 6.7953, val loss 6.7953\n",
            "Generated (64 tokens):\n",
            "\u0000;er. iI one problemhave been  corresponw, est a stt get eanalyOperbli'Thto in such as in the is ce of on’, als  the xsrespeces totion, pros onroes in ence ecis iprchleveles be  that cases ofn\n",
            "\n",
            "\n",
            "Step 1000: train loss 6.3574, val loss 6.3574\n",
            "Generated (64 tokens):\n",
            "\u0000the the of the sentences iials to correc, ande does not omessed still uling was term that fa, :\n",
            "one unto atselh. As in czs which koms in adsts is not not does in ae w in the of the believing l\n",
            "As was \n",
            "\n",
            "\n",
            "Step 1250: train loss 5.9318, val loss 5.9318\n",
            "Generated (64 tokens):\n",
            "\u0000our provides for requires in which the in the hde, the tes.\n",
            "But to flual for doing so of not that we the in ped of continging feet terms of philosophical from special bot in the sentence 'TA, in terms. We be its not to do not and of the not to \n",
            "\n",
            "\n",
            "Step 1500: train loss 5.5892, val loss 5.5892\n",
            "Generated (64 tokens):\n",
            "\u0000hypothesis consequence in philosophy has reacheely to make with\n",
            "P→ it does not entails would not to know is to make the now p for it was recognize mating in truth-valum of explanedition K by to make the first unaction' in \n",
            "\n",
            "\n",
            "Step 1750: train loss 5.3131, val loss 5.3131\n",
            "Generated (64 tokens):\n",
            "\u0000(Ct 'Consequent. Comey,' 'C\n",
            "Materom The the descriptions in the sense for how 'by more reshad: or to assrascription. Even evidence is hn of (12.4 0 R is to make the problem\n",
            "\n",
            "\n",
            "Step 2000: train loss 5.1349, val loss 5.1349\n",
            "Generated (64 tokens):\n",
            "\u0000lines in semantic not the (a priori ~K p & EF) If p & S), see A))\n",
            "(A)\n",
            "\n",
            "The Ay\n",
            "A) = (A) B) = 1)\n",
            "(2)) =  1) F p))\n",
            "(I\n",
            "Prob\n",
            "\n",
            "\n",
            "Step 2250: train loss 4.9852, val loss 4.9852\n",
            "Generated (64 tokens):\n",
            "\u0000in ' a risity-analytical-theory for a systemaxford to make the crealinguistic ck'I can' in the contential'One might be the unied. Similarly, for Sn by controw in philosophy and elimination as for\n",
            "\n",
            "\n",
            "Step 2500: train loss 4.8730, val loss 4.8730\n",
            "Generated (64 tokens):\n",
            "\u0000formula of its belief that it in 'That least to know' or not really like a use of containing' in ¬it does not know that it is raining our evidence that knowing ptainly unies\n",
            "A llumin include avidson is true, it was not content\n",
            "\n",
            "\n",
            "Step 2750: train loss 4.7828, val loss 4.7828\n",
            "Generated (64 tokens):\n",
            "\u0000safemale foxiger's act (2008), and so on that our evidence is to have to have accept. Wittgenuch's argument of the res). When Preplace in philosophy\n",
            "The knowledge as something is uninto the same losend of \n",
            "\n",
            "\n",
            "Step 3000: train loss 4.7293, val loss 4.7293\n",
            "Generated (64 tokens):\n",
            "\u0000cases are not to else the argument does not know that one know that one does not know that it is a priori has been know p one knows that we cannot be a priori can know p in which one of the worldsay it does not know what good case for examples in p (p & ~p) would be clock is a case \n",
            "\n",
            "\n",
            "Step 3250: train loss 4.6578, val loss 4.6578\n",
            "Generated (64 tokens):\n",
            "\u0000generals about knowledge of course is ink of philosophical tralists, or not just we cannot be the pointy of reasoning on its inconsists about such cases, the work of doing inchectual cases. In particularly overfitting to be unconscious of their fundames for \n",
            "\n",
            "\n",
            "Step 3500: train loss 4.6137, val loss 4.6137\n",
            "Generated (64 tokens):\n",
            "\u0000evidence in the bad case for instance, the presuspaccount as slivetigak of counterfactuals balinguishomic recognizing or not to realists, but falles of a reator 'gin f,'S'. One can know whether it \n",
            "\n",
            "\n",
            "Step 3750: train loss 4.5664, val loss 4.5664\n",
            "Generated (64 tokens):\n",
            "\u0000fer in a way to use the arguments in the bad case to recognizing dry, and so once it as 'analytic' in effect to provide and '. One can give the language offemale fox' have before in the good case for example' rather than for variabl\n",
            "\n",
            "\n",
            "Step 4000: train loss 4.5221, val loss 4.5221\n",
            "Generated (64 tokens):\n",
            "\u0000faxiombrigorse: 's'S\n",
            "A' for 't B'B' in Jack the same language of acch is belse' has y' to strengagrecognize to the question '. Finput inconsistent with the use the first principl\n",
            "\n",
            "\n",
            "Step 4250: train loss 4.4901, val loss 4.4901\n",
            "Generated (64 tokens):\n",
            "\u0000(for (3** in the bad case ire is not a bers when one could not equivalent of its the total world, his view, no more than rigorse as true in (2). Similarly, if one cannot be safaments, . . . , (2) are not refer is not equivalent\n",
            "\n",
            "\n",
            "Step 4500: train loss 4.4694, val loss 4.4694\n",
            "Generated (64 tokens):\n",
            "\u0000evidence is at least an unknown truth is the imman unighosphorused to that one is in Ra, even if you believe is to be the cred as the unlikely to Nax and truly, at least the relevant to the same mental state Some, a proposition p and Stal\n",
            "\n",
            "\n",
            "Step 4750: train loss 4.4308, val loss 4.4308\n",
            "Generated (64 tokens):\n",
            "\u0000testing inuents of the structure of the evidence of analogent of knowing.\n",
            "There is by represent purposes in effect from 'unusual operator 'Datic number of A fort' with 'Now' would be a semantics, for some speaker is some \n",
            "\n",
            "\n",
            "Step 5000: train loss 4.3751, val loss 4.3751\n",
            "Generated (64 tokens):\n",
            "\u0000mathematical example in terms of human instance (Williamson 20115) have a mental states and conditionalsince the intendremabody of them from the same mental state as an unity to realized by their impossible worlds as well as in more rest that ways, we may not reasoning it \n",
            "\n",
            "\n",
            "Step 5250: train loss 4.3841, val loss 4.3841\n",
            "Generated (64 tokens):\n",
            "\u0000such cases, the proof an Nather, when we might be the radich to handle to resete as knowledge of beenable to mchair of modal operatoms involves a brain every sentence is what one will have the relevant data s; it cannot be \n",
            "\n",
            "\n",
            "Step 5500: train loss 4.3572, val loss 4.3572\n",
            "Generated (64 tokens):\n",
            "\u0000evidence is needed once we cannot be to proof them that they have inities. Aculties about meaning. For the natural number of place of account as to its a of the relevant to helper of our evidence to representation of the relevant to reity as a rastic logici\n",
            "\n",
            "\n",
            "Step 5750: train loss 4.3237, val loss 4.3237\n",
            "Generated (64 tokens):\n",
            "\u0000in Glimp. Thus (p & q)) ⊃ p)\n",
            "= K → ◊(p ⊃ p))) and\n",
            "~Kq is ink p & KKp)\n",
            "B~K~Kpp); it may wnone knows that one \n",
            "\n",
            "\n",
            "Step 6000: train loss 4.2861, val loss 4.2861\n",
            "Generated (64 tokens):\n",
            "\u0000evidence in the same book of its conjunct a of science of a sentence 'She is Mars carer (1993). Since Barbitzer, 1: 236795840008335073673755–\n",
            "\n",
            "\n",
            "Step 6250: train loss 4.2753, val loss 4.2753\n",
            "Generated (64 tokens):\n",
            "\u0000way of reasonable propositions the compart ordinary case one might be the condition that I was a propositional to recognize it as infinite description of knowledge that is not a priori that it is a bal, the assumption without being in one's total logic-grounds? The treat least one \n",
            "\n",
            "\n",
            "Step 6500: train loss 4.2096, val loss 4.2096\n",
            "Generated (64 tokens):\n",
            "\u0000come (for their productivity of (2005). Pree Popincheckillul Universian (19793: 1174), and so ondreaccount as 'Jacked bile' in terms of Weink it was \n",
            "\n",
            "\n",
            "Step 6750: train loss 4.2022, val loss 4.2022\n",
            "Generated (64 tokens):\n",
            "\u0000of an inconsistency of their previous terms. For example, if one cannot be very knowledge, the argument is to dismissing them in the bad case is to one's beliefs. For example, if one believes p is not a mental state.\n",
            "One might be expects, but that one believes p, and p is a world in \n",
            "\n",
            "\n",
            "Step 7000: train loss 4.1780, val loss 4.1780\n",
            "Generated (64 tokens):\n",
            "\u0000replaces the problem of natural language. For example, I do not know p, it is written in every vixen that it is a mental condition that one does not know p, since it does not believe that the conjunct p is not just a p.\n",
            "We canon that p is true and p is true; one might expect p. Consi\n",
            "\n",
            "\n",
            "Step 7250: train loss 4.1584, val loss 4.1584\n",
            "Generated (64 tokens):\n",
            "\u0000replace it was that they do not reverify hot it does not know p, because the relevant to past or probability of p (p inches (p ∨ p & q):\n",
            "(7B)\n",
            "᭙p & q) & q is true at w is true if and only if it is \n",
            "\n",
            "\n",
            "Step 7500: train loss 4.1115, val loss 4.1115\n",
            "Generated (64 tokens):\n",
            "\u0000probabilities, for assertion in no difference between counterfactuals, not yielflowers and which he oup which one believes p, the f(as in other gin which they do not know p). If we know p and p is true.\n",
            "There is whether p is true and p is true in p. Consequ\n",
            "\n",
            "\n",
            "Step 7750: train loss 4.0736, val loss 4.0736\n",
            "Generated (64 tokens):\n",
            "\u0000number of cours' as ' applied with ', and 'drinking '. Chapter 3, Presumably, the hears will realizing the same ways, where the would also be the tuniquotation.\n",
            "Modality\n",
            "\n",
            "P. Consequently, Sim\n",
            "\n",
            "\n",
            "Step 8000: train loss 4.0443, val loss 4.0443\n",
            "Generated (64 tokens):\n",
            "\u0000evidence is quite different worldsets in those who do not seems to the two cases are not to far the premises in terms of knowledge that it is hard to have a contradiction between knowing an unclear to down to the action is. Thus, it is the consequent of meanings, \n",
            "\n",
            "\n",
            "Step 8250: train loss 4.0468, val loss 4.0468\n",
            "Generated (64 tokens):\n",
            "\u0000propositions as as to one's belief in one's narrow if and only if p is true if and only if p is true and p is true and p is true and one knows that if p is true and p is true in α is true at every world at a world w is true at w is true at w, so ◊(w)/C obtains in α − ~ \n",
            "\n",
            "\n",
            "Step 8500: train loss 4.0235, val loss 4.0235\n",
            "Generated (64 tokens):\n",
            "\u0000counterfactuals, but the controversial to which it in practice most of our knowledge and modality in philosophy, in which it may not find enough to test in order of knowledge and not to proper of their science.\n",
            "There is a famination to how find it in\n",
            "\n",
            "\n",
            "Step 8750: train loss 3.9752, val loss 3.9752\n",
            "Generated (64 tokens):\n",
            "\u0000deduction in fact (see Horwich 1999, 1999). In particular, his own examples him has been delementary to disposed to deny that it to reach the new influence of the left (2005). In particular, is also sm\n",
            "\n",
            "\n",
            "Step 9000: train loss 3.9433, val loss 3.9433\n",
            "Generated (64 tokens):\n",
            "\u0000content, with usefully to their rest the theoryield on present purposes in philosophy.\n",
            "Here are concepts, for the appeal to use of its proofs. But that is identifice and the best explanation of a more complex, because it can always or just \n",
            "\n",
            "\n",
            "Step 9250: train loss 3.9169, val loss 3.9169\n",
            "Generated (64 tokens):\n",
            "\u0000perceptual knowledge; we want rule. Of course, if one is not a priori tomorrowname 'Nood' or 'I do not retachieventually int it may leaderived by the hypothesis that the favoids of thought; it is unreli\n",
            "\n",
            "\n",
            "Step 9500: train loss 3.9103, val loss 3.9103\n",
            "Generated (64 tokens):\n",
            "\u0000mode of using its consequent to the original question 'analytic 'Mars was always either dry' in the book 'This is x is a part of utterance. A slowertainly expressing 'K A or B is true in the lottery at t might be explained in the sense \n",
            "\n",
            "\n",
            "Step 9750: train loss 3.9169, val loss 3.9169\n",
            "Generated (64 tokens):\n",
            "\u0000more combination of 'analytical' in effective or 'natural'. Similarly, I was not make it was not not just do not 'Hesperus is Phosphorus' is assummarginal framework of constitutes by 'Every vixen is a vixen' (19854\n",
            "\n",
            "\n",
            "Step 9999: train loss 3.8617, val loss 3.8617\n",
            "Generated (64 tokens):\n",
            "\u0000statement of the analysections. For instance, one of an external evidence is holds the difference of a given propositional attitudes us to say that Hesperus is Phosphorus. If you believe it nor not dren to make the speakers of the argument for why it up WVE\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Final generation (500 tokens):\n",
            "\u0000quantification to make the qualified a function. The idea that there is a god, not the account of thought is true belief without knowledge. That is an unlikely to chapter, or even if it has the argument does not follow that evidence that he is an?' have in \n",
            "\n",
            "Model saved to sim_williamson.pth\n",
            "\n",
            "Model saved to /content/drive/MyDrive/sim_williamson_1e4.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling/completion script\n",
        "print(\"Loading model for inference...\")\n",
        "model = GPTLanguageModel(VOCAB_SIZE)\n",
        "\n",
        "# Load state dict and handle potential _orig_mod prefix from torch.compile\n",
        "state_dict = torch.load('sim_williamson.pth')\n",
        "new_state_dict = {}\n",
        "for k, v in state_dict.items():\n",
        "    if k.startswith('_orig_mod.'):\n",
        "        new_state_dict[k[len('_orig_mod.'):]] = v\n",
        "    else:\n",
        "        new_state_dict[k] = v\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load BPE encoder\n",
        "import pickle\n",
        "# Update the path to load from Google Drive\n",
        "bpe_path = '/content/drive/MyDrive/williamson_bpe.pkl'\n",
        "with open(bpe_path, 'rb') as f:\n",
        "    bpe = pickle.load(f)\n",
        "print(f\"Loaded BPE encoder from {bpe_path}\")\n",
        "\n",
        "# Prefill prompt - EDIT THIS\n",
        "prompt = \"Knowledge is \"\n",
        "\n",
        "# Encode prompt\n",
        "tokens = bpe.encode(prompt)\n",
        "context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Tokens: {tokens[:20]}...\" if len(tokens) > 20 else f\"Tokens: {tokens}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Generate with different temperatures\n",
        "temperatures = [0.1, 0.5, 1.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nTemperature {temp}:\")\n",
        "    generated = model.generate(context, max_new_tokens=200, temperature=temp)\n",
        "    generated_text = bpe.decode(generated[0].tolist())\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Interactive mode\n",
        "print(\"\\n=== Interactive Mode ===\")\n",
        "while True:\n",
        "    user_prompt = input(\"\\nEnter prompt (or 'quit'): \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    tokens = bpe.encode(user_prompt)\n",
        "    context = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    temp = float(input(\"Temperature (0.1-2.0): \") or \"0.8\")\n",
        "    max_tokens = int(input(\"Max tokens (default 100): \") or \"100\")\n",
        "\n",
        "    generated = model.generate(context, max_new_tokens=max_tokens, temperature=temp)\n",
        "    generated_text = bpe.decode(generated[0].tolist())\n",
        "    print(\"\\n\" + generated_text)"
      ],
      "metadata": {
        "id": "mCF80hvQloqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94bf313e-75e5-451f-a593-5e42f5427d2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model for inference...\n",
            "Loaded BPE encoder from /content/drive/MyDrive/williamson_bpe.pkl\n",
            "Prompt: Knowledge is \n",
            "Tokens: [1965, 437, 403, 276]\n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 0.1:\n",
            "Knowledge is to recognize in Section 1.2 There is (1981: 2888–108). But that does not follow that Principle and Peter and Stephen is a fairs (2000a), and so on\n",
            "2017a: 212121212108-08-20213, 12.2 Replanism, Rosense the category of ' as ' as ' as 'There are literally true belief without knowledge' in believing that the proposition that it is raining. One might be a priori be a priori be a priori be a priori be a priori be a priori be a priori be a priori be a priori be a priori be a priori be a priori 0 p, . . . , . . . , . . . , . . . , . . . , . . . , . . . , . . . , e \n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 0.5:\n",
            "Knowledge is any.\n",
            "Thus the best explanations of thought experiments or less than the attitudes of another reason to prior to heuristic-personal science as well as to do better than to distance. The most previous – never realism on the chapter, it can be reatures of knowledge, not the proposition that P, was rained together's argument as 'if,' 'Hesperus is Phosphorus' is not a rule for all truths it the proposition that P, is Phosphorus, that the oppositively as part for some of the antecedent of (1*); (1) are not the below is the actual world, (1). Evidence within section 5.4.2\n",
            "Penny was a convenience in (12) 'P' the same object-grand' are in'\n",
            "and 'fit the best explanandmonkeeping so \n",
            "--------------------------------------------------\n",
            "\n",
            "Temperature 1.0:\n",
            "Knowledge is a form ofsture clarifytries to intension. It is to one’s explanations. Madstitutions in fit the who thinking in the book's fail to existent speaks of philosophers to written it to be sociated to my of applying some datoes a costot.\n",
            "Nazick's outlabsound for that indicates are one have stknow that p is a in which one is mosentions by independent that in pain itself (3*). He can even the pecially to the See then include. Wittgenstein’s idealized on the basis of the examples of sentence 1994). In 1, just the tree and some publither,\n",
            "expertise are to the first edition, thereas knowledge' in the internalism 's. Maryoore prectorture between (10) is the Simportion is the elknowledgments to section 7.2 or indutrat hing \n",
            "--------------------------------------------------\n",
            "\n",
            "=== Interactive Mode ===\n",
            "\n",
            "Enter prompt (or 'quit'): Wait, hang on! I'm a language model, and\n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Wait, hang on! I'm a language model, and a maskers of '. On the baldom ' the breaking is inent of his creature and ysts in the laws of amistakens of the initial reductive methodology of another (such as the bodi\n",
            "\n",
            "Enter prompt (or 'quit'): Wait, hang on! I'm a language model, and \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Wait, hang on! I'm a language model, and so on' the below was a of his borderline cases, which he was not yet the borderline cases, which he was not to his testing it as to make the back of his briefly b settonkey of\n",
            "\n",
            "Enter prompt (or 'quit'): Wait, hang on! I'm a language, model, and \n",
            "Temperature (0.1-2.0): 0.1\n",
            "Max tokens (default 100): 64\n",
            "\n",
            "Wait, hang on! I'm a language, model, and so is anything like the influence and not to have a left a lack of his tasking about a diamon’s tinvestigations: ' as 'the baldom ' as ' as ' as 'know' to 'know\n",
            "\n",
            "Enter prompt (or 'quit'): quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mxzEU7vzJF_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}